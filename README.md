# ETL--pipeline 

A data engineering project executed in **Google Colab** using **Python**, showcasing ETL flow for nested datasets.

##  TechStack
- **Python 3** (Colab)
- **Pandas**, **JSON**, **OpenPyXL**
- **Google Sheets** (final output)
- **GitHub** (version control)
- **APIs** (mocked/dummy for simulation)

##  Functions performed 
- **ETL pipeline**: Extract → Clean → Flatten → Export  
- **Nested data handling**, **data wrangling**
- **Output formatting** for analysis/reporting
- **reusable script** for recurring data jobs

##  Output
Final `.xlsx` output is included in data/ folder for downloading.

Link to view it in google sheets (https://docs.google.com/spreadsheets/d/1-cBE-msrOB70j3taI5L2IqfuxggX8x1R/edit?usp=drivesdk&ouid=108221278507136154331&rtpof=true&sd=true)


##  How to run
1. Go to the notebook/ folder.
2. Open the .ipynb files in numerical order.
3. Each file contains one code cell — copy the code from the preview section.
4. Paste each into a separate code cell in a new Google Colab notebook, in the same order.
5. Run the cells one by one to execute the full ETL pipeline.


## Purpose
 The project automates the end-to-end ETL (Extract, Transform, Load) using Python, public APIs, and Pandas. Raw Data was pulled from a JSON structure simulating real-world API responses to showcase extraction, transformation, and export into structured .xlsx files compatible with Excel and Google Sheets. It displays core data engineering practices like data normalization, transformation, and automated file export.
